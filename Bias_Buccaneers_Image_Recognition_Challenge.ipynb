{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lv5OYlsx7Vz"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/drive/MyDrive/data_bb1_img_recognition.zip /content/"
      ],
      "metadata": {
        "id": "mdIzGvkwMta3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip data_bb1_img_recognition.zip -d /content/dataset/"
      ],
      "metadata": {
        "id": "oMOks48_M2Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet101V2\n"
      ],
      "metadata": {
        "id": "vKgnknGKMq9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "LOADPATH = '/content/dataset/train/'\n",
        "SAVEPATH = '/content/models/'\n",
        "df = pd.read_csv(LOADPATH+'labels.csv')\n",
        "df_labeled = df[df[\"skin_tone\"].notna()] # take only labeled data\n",
        "\n",
        "# Converting labels to np array\n",
        "cat = ['skin_tone','gender','age']\n",
        "lbs = [LabelBinarizer() for i in range(3)]\n",
        "Y = []\n",
        "for i in range(3):\n",
        "    lab = lbs[i].fit_transform(df_labeled[cat[i]])\n",
        "    if lab.shape[1]==1:\n",
        "        Y.append(np.hstack((1-lab,lab)))\n",
        "    else:\n",
        "        Y.append(lab)"
      ],
      "metadata": {
        "id": "9aFjeTbpNXJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading and converting data into np array\n",
        "print(\"Loading images\")\n",
        "length = width = 64 # size for each input image, increase if you want\n",
        "nn = df_labeled.shape[0]\n",
        "all_imgs = [image.load_img(LOADPATH+df_labeled.iloc[i]['name'], target_size=(length,width)) for i in range(nn)]\n",
        "\n",
        "print(\"Converting images to np array\")\n",
        "X = np.empty([nn, length, width, 3], dtype=float)\n",
        "for i in range(nn):\n",
        "    X[i,:] = image.img_to_array(all_imgs[i])\n",
        "X = K.applications.resnet_v2.preprocess_input(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKVO-pTBScot",
        "outputId": "c93577a5-e9fb-477d-8e16-d3e470a72af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images\n",
            "Converting images to np array\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_imgs[1234] # print a test image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "QGOff17MTN7Q",
        "outputId": "db3b9b73-dadd-4358-d209-7a6b6bb432c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F5A384BB040>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAjPElEQVR4nHV6WZCc13XeOffef+l9ZrpnxSzYd4AbKC6QSFOitZCWrNiWLdkq60FeKk7ZSsUVJS9+kZ1yJalUnEpcdlxOObLsKLYsKVpKsiVKJEiCG0AQK4GZATD7PtP7v93lnDz0AKaq7P+pq+vv6rN859xz7vfhhStzQggAcI6tc0RARERMREwIAACMKIQEiSilVB4GvvI9IaXA3gMIANZZa7ibuo3t7o//4UdZs90feNeuX/rsL35qeGiwub2dKxXq9fqeQwe9Qk6gkEKkcaJ8r1QpG9K+75UrRRTASMQMCIDIAAIAGRgYEZm4918AQCAAHQLgP+0AAzn3TzigRKHo+57wFCKCQISeCwDMbJnJuCh1Ude9c/Ha3Ftvf+RjH9Sdth8EiKIw0G+N227UC2Hu1vTtUqFokmx5afHw/gPgbBAGcdx16CqDQ/WtOjCX+/u8fMg2y/eV9h47UKr2AZBAAkJGAono4J9zgJmZiMgBADCDEIgCBHAYBsWy73tSSUYEBLwfEgAgYGAwhpM401G2M7/mS9HY3C729RcLlRdfeeXkkaNf//Z3Tz1wqj9fmrk922i1jh06PHv79k5r59D+vX25omBO02h8fOLu3TkA9P2gOjgchuH21pYzJl8sNLstZk51OjIy2Dc5dOzUCbxwdV4gAjARW8fOMXMPRbsOAAgAEogoIZ/3+8qeUkJJAQCAAgAks0PoJdmxiFvpwtVbNy5ctlpncaZy3r7JyZmbt2eWlgZqtb1TU9954YWHTjwQtdt+Pn/t5vXnnn/uy1/+civq/Nvf+K3ZmbvFXDCQCwb3DPmF3JU3LmnEMJdPkmxiz3i92Wi12/19ZSICZmbrCcQLl+dQICL8cw4wIwChQCEgn/er/aFSQvSww4AADoGFYM3d1cal82+1NxpKeEkW37lzR5v48bNPoLB/9Wdf6du/tzYwPH1ntlYbTi305QsSGXPBj199+eFjp7W1a43Nj5x9enVjfXb6xofPnl2eu/Phj374xVdeB5SpMdbQYLW2Z3x8Y31FCDE9PX1o//5SpYBvXb4rpQQAIuo50IOQc+4nHEAUEorFsNofSolS7OJeCNnudob6R7/yP/68s7RtHWtnd3a2duo7/X19Widnnnjk6LEDnoW//vo3bs3clX7gFSqAcnRkpKSC2CQXbl6bGplSLLsivXntyq9/5vN/971v1QYHnn74getvvvmhD3/o+o1pRzJfKGxsbsdpCgJ2dnYqlcrx0yeW78wLZmBmAABAZgTYtaz3JTMzO2ZkBiZgcuJ+6BnJObCwPV9/5Zs/6OeAbDYyPCBI66wbBjC1Z+DUyUODtUq5Ura+O7J3bM9I+ejExPLW0nZz89q7V85fvtjcWncmTW3U118wnY6V9Oblt545e7bRaHz75XND+/ZdvnRxZLjiYdKOG5MTI5VSYG2a2eTYsUMvvfHKzMYCvvnOnBAghHCuByEiImZ2lnsfmBlAILKUslj2h6o51at6wDh1b59786/++C9OHDs4XBsoKEhMNjQyVK1VjbF+LgdCMEKlNoASVq5f//GL5wj8W8srHYtGG79U3D80cGN2FlU4WKkRcCuJjDGlfH5yavLi5Uu5QD1x5LCJo8HB2qXLN2QufPyxx69evZorFBaWl06fOPHC+VfxjUt3hRBCABEYS0RMjojAOUfEzMDseg4gYqU/NzwQKiEtAhPcuXT7f//xX0yNj5bz6tTRA+XBqkMolIuMgAzEZI1lR6gEIeutzZW7cz986eWJ0fHvXb6Ont9uN0erg3vHhmaX1/ygqI3xpNdoNSf3712cXxgZGbm7MLd3z3A1CHOhfPDkiW987/uFYvnIwcPTMzMsxXa98bGzTwmm+1D5iacHql4b+scvmRmQAKRQ6Wbne9/67qXpa/Wo+exHPlg7sjc/PNy/Z8wrltAP2FMghJRSCuEy4zQFpcrU3qlHTp6slgon94xnSXJk/4G1rZ3telM5vba+mOhoe3sNlVtanlOFYGF9JRfmFja2NEClXLlw+cIv/8InV1cXl1eX+iqlci7s6+v/mx99TxABkXOMxPRe299jOTICoATB2OuqxJ2Nzf/5H//owjuXP/1LP/trv/krspIP8gURKBCC2WGvfgiYmKwDrSmOkMEi72yukM6OTgxKdJ12O8xh3URnHn2oHIbEJH0cHai26vVWayeK2zJQvpQ7SXLx3ZvVwZHXXz//73/ntzY314UQlVKhr5wv5XLCMjlCco75XkHzffN3PyADE/UmByHBpunLX/t/79559yNPP/mzz31Y+j4ID9FDZmsN9crIWHCOrHUmQ2bMsk694ZiffOangF3WbB4dGoyyKO+Hyyurr1+8eObB40XP83x/fX3N9yQZozzR7DYLhbwfqLbWr128FGt66dXzP/czzy+szHfiaHywtnd4WJAj54gZd0PP4r1dqGc9ACAiOwLHZPmlr37t/37n22Tcz3z8o+hJco6ccZRZbSQjEoNzZKzVmqxDBrJGChLOKfAJcOrg/v6+0tHRoX5faZPsGx2ud9LZuds5xUooEcix8XFgcsays81u0xkLCPU4DgvFmbtz166/c3jf5M07s/V2c2LPmNhtl/fDjgTwHhABkOhByxbCnHF66c2L3/v7b+tM/+7v/g576JwTQgAQOBIKiS0hEDP1TkQAw05KNJlzNnNxxyfOhTn0ZVHh4Wq/VCgY+orhVqudC/3N9aVOktQ3N2rVmgqUtSZKo7ml+TRNq9XBt99999ihY8hw/OiRoVrf1dvT04tz4l51IvyzjxBMxTBItFHaXjx/bnZ58+iRw7WRGjMzcc8HBmBHzGydA4HS85SnQKDv+zozTmc2Tcnobr0OOnvwzGNawnAhPFSrpmmS89ESrDVaj515hNlocDZLXDf1lOeMBSGEEM1WSwbhSqPeaMcK8KMfOFss5Fc21nczgO+xnwHuD5jMjAxSCK2NEjzz2kvf//GPP/0vfuG3f+e3g3If9mZVACbqVQw5klJKzxNKCqmklFL6IARI9P2ADDljkihqNrYffvLJweHayZHhqhSjlUp/KDOduCQ+ODho0ijJ4j2DgwI4DELf88NCPheEnXZ7dn5heGzP21dvTM/e+dSzzw7194v7oAeAe6MlM/5jSpDRMTqWyqXTr7988tCxZ57/GBbyxjoCIGZwZB0rVOSc9AMhFDpSQmbGkCOrNSAKFYDnQz7wwrwf5mycVvprh0+fLghx5vDBPilPje85ODF2e2l5rL9vpFx0YAXy/rFRtI6cXd/YDHO5kaFhB/z2u+86FGlmrly//qH3PS7ei5VeJfD9RPQckI4ZQkXr197WIH79i18MS2XQrAhBW5dqZxwydLuRtb0pipy2OkuVlCgF+BIDXxWL+VIlzJfzgyNB/6CfK+QKueWl1T2HD3gCfCXKuZKfmmqlvNluHNwzOVKprNfXC0E4UhsohKHyxFZrpx11Kn2VThQ3Op3tbmSIFpbm1f2GeR9EArC3Et3LgCJwEEWbK5sf/dRn12YWyFnP41yQM1nGgpkw11/2AG2zzcW8SU3W7ZLR3XaHNGxsb0SJ9dDLnJaeoCjKh6FQsLj+6kOPnbl77drg8Egu133n9u2RoWG3tpZoK5QYKhSTJLl+8/pgrepJQHaZzjqddjfujo+MLa9vZDoxY6N5X6mfSMBPpIMRHACDpLwP2zOrlcGRq5dutjY2Gq2tSk7sGx8bq42IfE4qBUpkSZQ0mioMKcuSKK6n5pOf+fyf/PlXHjr7wbfOXRyuDtbvzPuj+Sefeb5a8H7wvW9yRzfaUSfWk1P75udvHxgfX9za2jc53u0kF25dP3Ho8KHRsUvZQjeLAuH7UtQ7HS/wLbvV9dUDE1P1+vbd5aV9ExP4w3M3PV8qKQGJCJ1lImONKZVKQkI+n+t2k9bael8+b0nvPXrIS8QXfu2zlNY94GfPvD+X90fGh/Oje7prG2m3I7LM+uEDH/rEN771wlM//czM3aXN5WWy/MjZh+9en/7Yrzz/0td/JEtq4tC+IvNX/9t/f/8nnj33/e8/9fij22urf//q+Yzh8NRUauwbN64c2n9gdXPr5vJyudRfKuQW19dT56TAUAbVUskTIibdrO8IZmZCAseMTAhIlb7i+MRItVZGdFG3FQhz5Ni+1EY6dgL4wsWL/+5Lf3h4/MRzP/vLB555dnozXs8Ki7E8d2M27urmVnfgyJnple3nfu0zb715obGy9PhjDx86PDE9c3VictQaB3nq8yvpduQXKmqwdv2VN558+uz00mJ/rfbo6WND5ULaaeaQHjp67PbC3LGpyb1Dw92kG8fpI8dPlcNQShnpeLvd9IJgfGhkeGhISMEIxLtjDpUrudpgWZu4020MDdf80C+Vihdeekeo3FI3eWN60eX8ty/cefZTn71y+eLt8y9XRSq2bo9S932H9//Bl/92zzMfy8gL88V3vv3jT3/uF4N8Ye+JI39//pZfHMtE8bs/eOuBp85uNHaWb9558aVXT73viXakv/bVb4yP7at3zeTEvvGhIV/6/eVCRYqRgdrSymrJw34/3Gk3bs/dPr7vgMfcX+kzzs6tLLSiDjHgS+enQSACkxCBB0ODFXRup9WVfqgEAPPrr105deTEi+df66/Vrl2edzZ938kHn3jf5Lm/+/7shR88cPRomnQe/8TH26tbW5l0mOOC2lndHtwz3Gm2pOdN7D306osXnnzumXJ/fmFp+dbs4sLCpmjX//XnP+uX+OK5S1z0XnrhB0899qDp7owP9r3x8jnle61ut1Due/XK5eGR4dn5hfVOV3nhYH81XyrOzt0R0lOe50uR83zlB8o6Zy2MDPW/8+rLfe9/HzkaGOhrtrqlcjlrdmZefv38335ramTyr69/80M/83Nnn3gG0/pr//Cj0VKwyLK+sSkrpTTTjcbW08/8/NXFjdXOxoPPPpY2aN9Rr1ga+tP/+qeTh4+/8vq1hx8+NFyr2jTzYv13F1+/cvnmd3/wtcbW7EDO05q/uXDj6Q8+dePO9uHjxy+9+eb+wweXlpbfd/r0xevXxqo1Vt7cxqYhN+V744PDy9ubwGKrXp8aGxdh6Hm+8D31e7//B912J5fLG2OyLCOiqNOt5Uu+ab7/iUOnnjzyl1/5k1/87HOQNo4/cNB2m0vT1wYq1QMPPfTUh59t3LzWvb3wztJifrTYXxgQjJfOvfbGubdmL9+68vrrly69WqtgOc+UJHsqA2Vwf/Zf/sPYqT2X37m42mjeXN20SMz2tfOvVwcnX3nz7QMHDs/OzA7Wqu3NzfFqrdNt18rlkf5+Y7Jmp1EbGNg3PNpsNgr5Qr1ex+uzy1lqk0Tnc0U/ICmw3minUToyPiJanf/zp//r87/3b5Zn7lx68bWV+dWTJ07mgcPBvo25u2F14MTpE2ZxceXa2+tbG3Nzm5XHPvLIc0931re7zbZ0/PhHfmru0vXG1nJ/bVB34tW5JeOcIFiYXynVaj/9iY984Xd/s21aofByOS9UXqC8wYHq6YceSrY2RvtKCwt3+6q16bn5jrPrjaYmXG43om48OTI03D+4uLnR6HQAUPgKPR89T0kppSfDnF+t9efzxbiRrt2Z21pc+ssv/ZEz/Mnf+Ny/+uIXqoN9ohTuqfQR8InTRzq3bs68fn5pdXmz0RSham9t7KnWiuVi2m7Hzca5r369u7OVC4OtzbWFrZUPfuYTe49MlQb8gYr38c998j///u/t2zN0eHS8v1TKKQ+ZtM7anY7NMvS916++c/D4MU+4vaPDtXIBrK5WSnkhSpXC5taO0frE3n0D/X2FYh7vLq0Z47qRs5YGB8vOZpmlbjfh1C6/9dbSzO1nf/WXQ8//T1/60uc+8ytxoo8f3v/aK6+ceuyh+u3Z1UtXltfWm1mXtRsdGR9+8pmt+dXhwwfA6Z2Nzf6BAV+pxz749Oz1W4lNKUrjbpdtapSn4+zujUtSuDRO0izLdBolkZQiyBWNNadOnbhy5Qra7JmzT+ik046TG7fnZtc2+oaGL757PSyVsm58+tDhTpQkZJREAQqUgnYrpv48kfGFj85hll6+eFEoaZzemF/6whe/mGVZQO6lcy8+cObhzvzS1uzC6tpGqjUzSs9Ls+jJB078zZXLd9+pD1RrlbHB0ZHR1sb2nZlp8rlP5eNCEBb9xLjFmfmBnDx5+qiJI5NlJkmtztqtljE6yjIs5FyW5fwAgvCl1y48+9RjSqlHjx3dbnVaW1sPHjh06fZssVi6vbDwyLHj2+02Lq5sMnOsaXF+q9vZOfPoCe1c1EzsTnNp+tba9s4jH3jStLrsS4kCrQuV0vV6urqysbby7s13kyxjZiVlIFW+UB49/aDrpKW9o31jY57ncStmX6ESZDWigiC3vbBYzYcMhpwjnZg01VlG1tg0aTaaW9s7mkFbA1Js13cGBqrFUB3cP45sjYa3b83cmF9UxfLdtRWBrAiOHDwkeqO/FCyUHKiOkZWeUO319W/9+V+4pHvq9EnBGOTzgfRyhTAf+Nn6Rnt2dnXu7trycjPubndaW61GlKXbnXYQCDa6NDUijIPMQJZhIEJfCSbPD6sjYxd+8NLE4CB6DpElAgBIKTwp0ZGOkyxOPM/rNJvrG+vGWhBiY2vz7tLq/N3luZl5RD5z/NAvPfPUmX1TBeZyuS9lnlta3B3mlFBCgHHm7u2FWrWvvdHKF4uRseVimVh2m51KOc+W40Zz9eq11fnZpsu0xVKuMNRfDYMQAO4uzi8srdXGJqTRJ86+f/HOPEuJDKD84b2TUSNaeef6k2cf0FHd6dR2I6Mzm2VZEpssS1NtrY7iuBVFmbPGucy5RqedZFk5X7y1unpoZHR5fg2kGJkYmxwo/+pTH0j84MXr15ZX13BpdYuZLbnllYbJOF1fSlfn33jt/LEzDzUTTNrp8x/+kFAQd5v1i1e69e2bc/Pz2xuGbTFXKOXzpWK+EAZk3fb29lajvu/A4TNPPFmo9JNQuhMpz0elfD8gNoHEzs5mt9lAclmcpEkihGp1O8ZZZsyypNntdNOsncaRMahkN4oZGVDoLHnk5CmKU2208tRgf1+t2l8sFpqdtleuqN7e6BwwKEgaYbSdtlYODFYKJhk/+shAtWKzxFkLRt9ZWVydn/eCnPRkMzPr2xuFfH6UBgZ0KB0ZZxM2s3dnH3zkETIphWF+cCDZqetOg5UwxkTOdFsNHUcKRbPT1Vo7glRnUZaykM4Zba1xlgAMs040CZno2AAD0VajoeNkz+hop9O5Nj2zb99kIQw6cfdALre7kTm2yiRmdUnq6NbMrclDR43wCpVBYzKrU2esRPngU+8/9eTjWZr0eTkgapt0envzGxffeOHKpSjO4lQnxpb7+nsRUVIwOL8QeIECQ7YbdXZ2bJoIgizLEFFIachpZ5vdTjeOoziNs8wwpcYQOQLnnAFAZDZkV+t1CsPF9Y1mp5MBX781vby6XswVXjz/pgIAx8RWdlcWUNrp61dGJ/YW+nKitleE4LUdIfvSz6wuDI8d3zMxcfK0bbd++O3vjGZDicniOLYmm1lbMTo7dfjoo489VqxWZZhTQhmtOU3A6FTHOot0EpFzmc6IOc7STpSAVPVOGyWmOrPkNLvMWZRIxllnGdA5C1IAQ7PT1ZlB5P1jY4GxGzrb6HRMlh2ZmlKOwTmOG40sjiqhTwL2nnhgdWNn4uEHutsbJYlK+U6AXyp4hZxhKNb621o//+lPz92a2Vless56vu8HXqlUCkulQqkIUhCwcQad7tTrCrjbaDmbCcTMmKgbEYImZolxmhCgIU6NNuQMO0Z0vesFgeQYEK1zDKCtKeRyIHGxvm1THeT8VpqgEN72hjLaaW0270zXqtX1mev7znxABvnigJBStTbXylP7ZOADIHqeZRAAaaubK5XYlxMHpwoFn60VQgSeStIkzAcCgU1Gzni+z2wFmSxKARwSWWujKAIpjNGJsYnWhsiQM9ZZJgIGAGet692REBljPd8DJiLyfS+O4+rwcL3ZGuirtJt151wul7u5uKS0JZcYkybtNDaowoGR7uZKaXiCsyQolVF5QA5kQFJKgLTdEX4Q+F6nVc/iJBcEGhGBjDW+p1yUpNFWONBXGhpKu7EPBAxJ0jWpjpNutxs7JgKOUy2UZ41LnLXOpWkqfGENEzMKYa215AQKKSUzszaBUoieBpMLgijuxDYZ7htYXFttxfH+0TFltItb7XxYWJ2/XRsaFkplxvXn843NDd9TwGSSyC+q9pU5YUza6bbX11rza0E+r2qVfK3iwDmXxXFbCNCZqdfruIgnHnpI5cJOq5V2uybLtMmMMaVSKUriOEuFEM45YmLrPE+pQiHWmRAsURhjrLWASEQAYK21wGytA+eIFleX94yMLm2s3V5aOLzvwJ35uVsrSyrT1qSGjQ1kj+hARySUR1mSWU1h0JqbL+TKstVN6o3L515LAcJifv/oyNba9ur8UqIT8KA0UE5tmlAWJZm2Zuf8q09/4P1sTBbHWZZ044SYo3bTMqdGOwZLQAgohZLSOgcA9whzFEK4HsdCzMxCCGKWQpos09b2aGYhxOzs7N69k3cWVxRZa61DMmQSdpadESgBgG3GOrNxPHvlnf0j+9965a2xUl/bamO0debuzK3rC3OLnfrUoYPbrbpclMqTtYGyJepmKdXrfVevP3j0KABahtQYY430PGut9FQSJ5qAiMJcmGltrNlt5c46IgBAACWlsZlSigUSs3UsEIk5TpLA90v5AjKsb24+evSEYka/UGzPNV3UsllVdzu+72fGMHDc7g4O9Cfd1rdfeuFffukPyeEZY0FC0m5jFB9ZWmytbq7vbN/1wrV2Y3p96fbOlhAiyPme771w9erExJifC6nVMtYK5WtrnWNLZIkYBQBonWmjnSPjHAtkBmBG7BGKTnlSCOydCFpbxl3CItMaAEq5vHZ2Zu62YhAqzGkQOmoJp4kgCHzBSCDDMJBhbmCgOjh1uLu8xFGaNlsmShr1+urS8k6nFQOlQNrZWrFckPsjn7pRHJmkG3UjchpFpb/Y3KknmRbMnU4nzOcAQHjKWQKBvW7DTEKI1GhEKYSQgCDQESEQIhI5RJRSEpO1tge2MBcCQRR3Qj9UBAQSipP7N2fe7jTrY1PHNrc2Q0dEoIBZyuHJPQ0hKRRby2v1tY0sThJn9IDHXiCsyQusYA6YnSwoYGQMPBV4qlKpTIyOGa3R98N8rtGNiuVylKQ9it04q4R0zgEiIZAlKSWiAAQmB4yekEjgB6GxcY9qYGYllSMLAFE3UijKQb4c5hUSgsLS6FR56tjmzNXR4w8jkcuMXypJoHZzp1AbVkI54Oq+8Xx/2WTWGd1pt+rbm1ab++0CiROdhWEYBF61Wu2vDhBw2omkUuVyuZukzlE+DOM0tUyI6JhQCrACrEMAT0jojfaIAJaVIg3lXKndjZC4d1dLwEopYyw5R56ITVYKQ8XACGgFTj35vGl3tu7c6tt7PIp2ioW+OI5aO63BgfJQf81oYxPyfV84BhkoLHmCsiQxxjjrnLVZmvlBwEz5XC5L0yxJo25XMDrnEDEMQ0ektQZm51xmtFKSiIWUkiQz7YprhEByACAAcoX81s424D3uC4CZlfKlVNqYbhzHpKMk3d0HBEvyafKZj6+8/iIlLWddUBnueF6+XBHFiiqULXR9z8/1DVCasLXNjU1nQaCUaWKlTYgckVKeI0riWClV395GRN/3kyxN09Qa65jukSkopZTSQ3TAQikFTIxgyUkpgdFa65xzlBEAAgghJGKPwtBaa2MckAXyhe8ABEGPmiR0iPni5JMfsrKgm420s9o/NCTDYqcbkae8YtkrlghRO4yihC1JJVl55dqwF+TyuVylVBLsPIU6s0msu3EadZNWo+MsO8NElsk5Z1AIAPCVJwXmwgIRoRCIyMwCpc6MYwCULBT1yB8CKaUSSERiV9zDAlCiCJRUSojetToxAzCCdH6Ymzw0ePqxVrPNjspDQ9rYIAyYySaJbXdNHPnSY4m5UmFgoCqVVyxVgiAPiAIFEXuehwjGGG2MNjrLMkfOETGw8JRjCsPQMTFAHMeMYJwlYEdsrAEE51zP8nusrpCyd8RKZibnpJCI6CnlgAFgd6ERgASM5IDRgaCgr+/Qw92t5ZKU1dFRZyx1Oy5JyTkgSpIIMmcVMYCz1mjN5ALlZ5gKFFE3FkKAFKnOlFTkqCeLIiJCICLjLCJmWWYJeqoLIt4NIiL0ChmRmaWUiGjJ9Roo9I4IZikkA/tCOWDV42eop1QRyOQYmIliFlAejJOOpyBrtzlNJRGxkZLJEzL0pBA6SVBnLkus1nESx5nWWkvfS7IULABAqjNmttY6FJm1xAAAZK1jAqE8TxhtHDkCtNYiIgM453pKNykEOSLg3UgjOHICmZARCQGAwBPiHxmanjyD6d6JCNKAh35R6K4hC9bYLGNjjNY202SdtaZZbxitnc7q9Xo3jTUL4yxonWRpD5FCSGJy1hpriXrNHh0TAFhnwd1j1BGVUozQe4eZACHTupcHJBZCOCJEEEIgM/TUSoAohXpPk7pPMiExADtmzJz0vJB1ZOOYjU6TRBBHzZY1hrXutNrNdkdb3U1TTZQx9eYwS2TJSUApZe+gcMAg0FiLiEQkAJnZEQkphfJA7GYAEQkdk0OJni8tExhCZGBG6BUxco/LQCSBxtr7GUAABhYAjnvKJyIlwBkrlGcdt1st6XSSZFmS6jRz1jYb9Sw1nTg2QN007RjDQL0FtVeUSiDYnqoRBSAQ9xCvlGLnpFQCdrVt1jqllO2hl5lgVxhDzvWmIwnAiEpK5wiVJN7VMhGRun9GAACD6/0UwDEiMfXkgAmptqF4pxm1u9ZknU47zYx2WjunndXGWEeEYOi+8gt6xSeFYGYBYJ0TQiACMvQq0jlH95QlDGzIAvdkb87RrkxBokAJxIyIpDUgSs9zPckfAiAT0m4XwvdS9YhEIAAkovIEOeMYrk1Pv3vl8plHHl1d30FEY6wjZ50BZksAArW1PXOBgYEAsCc76KkQlFIILISw1kole8Hj3cbCxCwYHJNxdjcG2EMESKmAnDWmd/w5R1JK5xgYAEFK+V6a9T0EK/doe/YDpRuxAuhE7ZTTc2++5LQ9eOhooVpK2hFnxEToDDFJAGAQiMpTzgkplTEp3BNTOWeVUqnOBKC1loFRIBE5YCISDAahVy3ATOSYmQX2FMSZzoIwBADaHUu5B0smIiIFJEDeQxELZgvA0lPkjCdACRmR8yhdX1sp5XJoySv5CxsrM+8sVcr9K2tr2mirXZjzGVw1KP/8cx/bWltFK8GTkp0XBkKIerNpjDHOIUogQ9b2pIM9vAuBuqdy6wk0EFzPPdcTTULg+eQcIvaOWwSQgO4eFS96dXM/9tDTxzEjoucpZ6wnpTEm7nYFQ6VcLpcrq+sbpWJfFEWEzAJV4GfGWcLFqPWdH/3wsQce2js8mgephECiqNP1lQLkXC4UClFKLwgkQKA8MtZXnkDcXSkRAaCnNmIGBhQo8Z4xzIz3xNr3XwYAdQ9wPW8BAIhJSXTaOWDBjIidTjvLsr5inoiUF3phrh11gzDoDY7SC7y8AgSptZXy8ImTYRj27/RFUVSt1a7evJErFJdWlhRK5XtxpgcHats7q0RcKBT27du/srLS7nZ7zQcRiEgpBQBCCCJC3N2SEVFbi0I65xzsVj8i/oTYA5CEEEKgMVkuDJE5S7UUYnFubmLP3jRx1lChUGI2kj0UClEU+kf6hqZyQVkxKhQlz0drxqcmi4PVodFRsnrPQDUH2B+WhvoGkGn/5N4oihBkqVDKeeHK8kocRfKeCr5nkITdIVwpJaUUSnqeJ6X0ldpd06A32QrE9xTxrgScmJkDzyPnnHUCmMncuHFZgvVzYb5SaDryVCWXS+abHX/qbF91sgpZ1ppd34gFwsSecWYWUg4PDkXttlXSpFkQ5McnpzY2N6wzY8NDO9ub+TA/OjwaRdHG1oanfEIC65CZ7vVDqRQ517saQoHQa8z3sMPEuyBy/P8B5okHm7oFM+4AAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictionModel():\n",
        "    def __init__(self, X, Y, idx):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.idx = idx\n",
        "        self.trainX, self.testX = X[idx[0],:], X[idx[1],:]\n",
        "        self.trainY, self.testY = [Y[i][idx[0],:] for i in range(3)], [Y[i][idx[1],:] for i in range(3)]\n",
        "        self.cat = ['skin_tone','gender','age']\n",
        "        self.loss = ['categorical_crossentropy' for i in range(3)]\n",
        "        self.metrics = [['accuracy'] for i in range(3)]\n",
        "        self.models = [None]*3\n",
        "\n",
        "    # train a model specific for a certain class index in self.cat\n",
        "    def fit(self, index, model, epochs=5, batch_size=16, save=False, save_location=None, verbose=1):\n",
        "        \n",
        "        if verbose: print(\"Training model for \"+self.cat[index])\n",
        "        model.add(K.layers.Dense(self.trainY[index].shape[1], activation='softmax'))\n",
        "        model.compile(loss=self.loss[index], optimizer='Adam', metrics=self.metrics[index])\n",
        "        model.fit(\n",
        "            self.trainX, self.trainY[index], \n",
        "            validation_data=(self.testX,self.testY[index]), \n",
        "            batch_size=batch_size, epochs=epochs, verbose=verbose\n",
        "        )\n",
        "        if save:\n",
        "            if os.path.exists(SAVEPATH)==False:\n",
        "                print('save location '+SAVEPATH+' did not exist. creating')\n",
        "                os.makedirs(SAVEPATH)\n",
        "            SAVE_LOCATION = save_location+'model_'+cat[index]+'.h5'\n",
        "            print(\"saving model at \"+SAVE_LOCATION)\n",
        "            model.save(SAVE_LOCATION)\n",
        "        self.models[index] = model\n",
        "            \n",
        "    def predict(self, newX):\n",
        "        predictions = [model.predict(newX) for model in self.models]\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "kfHk1xG0bNAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to initialize a model\n",
        "def initializeModel():\n",
        "    res_model = ResNet101V2(include_top=False, weights='imagenet', input_tensor=K.Input(shape=[length,width,3]))\n",
        "\n",
        "    # # freeze all but the last layer\n",
        "    for layer in res_model.layers[:376]:\n",
        "      layer.trainable = False\n",
        "    model = K.models.Sequential()\n",
        "    model.add(res_model)\n",
        "    model.add(K.layers.Flatten())\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(256, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0.5))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(128, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0.5))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(64, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0.5))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    return model\n",
        "\n",
        "nntrain = int(0.7*nn)\n",
        "np.random.seed(42)\n",
        "indices = np.random.permutation(nn)\n",
        "train_idx, test_idx = indices[:nntrain], indices[nntrain:]\n",
        "mymodel = PredictionModel(X=X, Y=Y, idx=[train_idx,test_idx])\n",
        "\n",
        "# train model\n",
        "for i in range(3):\n",
        "    mymodel.fit(index=i, model=initializeModel(), epochs=25, save=True, save_location=SAVEPATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRbjLLoMbO77",
        "outputId": "d3da9fa8-0cb5-48f7-f748-7cba63628f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for skin_tone\n",
            "Epoch 1/25\n",
            "375/375 [==============================] - 23s 38ms/step - loss: 2.5966 - accuracy: 0.1360 - val_loss: 2.1146 - val_accuracy: 0.2042\n",
            "Epoch 2/25\n",
            "375/375 [==============================] - 13s 36ms/step - loss: 2.2748 - accuracy: 0.1669 - val_loss: 2.0776 - val_accuracy: 0.2054\n",
            "Epoch 3/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 2.1605 - accuracy: 0.1856 - val_loss: 2.0610 - val_accuracy: 0.2054\n",
            "Epoch 4/25\n",
            "375/375 [==============================] - 13s 36ms/step - loss: 2.1094 - accuracy: 0.1809 - val_loss: 2.0401 - val_accuracy: 0.2132\n",
            "Epoch 5/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 2.0797 - accuracy: 0.1964 - val_loss: 2.0319 - val_accuracy: 0.2155\n",
            "Epoch 6/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 2.0598 - accuracy: 0.2038 - val_loss: 2.0225 - val_accuracy: 0.2171\n",
            "Epoch 7/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 2.0472 - accuracy: 0.2098 - val_loss: 2.0136 - val_accuracy: 0.2178\n",
            "Epoch 8/25\n",
            "375/375 [==============================] - 14s 38ms/step - loss: 2.0403 - accuracy: 0.2110 - val_loss: 2.0100 - val_accuracy: 0.2182\n",
            "Epoch 9/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 2.0196 - accuracy: 0.2193 - val_loss: 1.9942 - val_accuracy: 0.2268\n",
            "Epoch 10/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.9945 - accuracy: 0.2282 - val_loss: 1.9991 - val_accuracy: 0.2175\n",
            "Epoch 11/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 1.9810 - accuracy: 0.2232 - val_loss: 2.0147 - val_accuracy: 0.2136\n",
            "Epoch 12/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.9680 - accuracy: 0.2305 - val_loss: 2.0074 - val_accuracy: 0.2167\n",
            "Epoch 13/25\n",
            "375/375 [==============================] - 11s 31ms/step - loss: 1.9633 - accuracy: 0.2215 - val_loss: 2.0052 - val_accuracy: 0.2194\n",
            "Epoch 14/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.9388 - accuracy: 0.2318 - val_loss: 2.0145 - val_accuracy: 0.2225\n",
            "Epoch 15/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.9132 - accuracy: 0.2586 - val_loss: 2.0217 - val_accuracy: 0.2194\n",
            "Epoch 16/25\n",
            "375/375 [==============================] - 11s 31ms/step - loss: 1.8963 - accuracy: 0.2542 - val_loss: 2.0184 - val_accuracy: 0.2229\n",
            "Epoch 17/25\n",
            "375/375 [==============================] - 11s 31ms/step - loss: 1.8712 - accuracy: 0.2629 - val_loss: 2.0606 - val_accuracy: 0.2124\n",
            "Epoch 18/25\n",
            "375/375 [==============================] - 13s 36ms/step - loss: 1.8633 - accuracy: 0.2686 - val_loss: 2.0574 - val_accuracy: 0.2221\n",
            "Epoch 19/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.8452 - accuracy: 0.2761 - val_loss: 2.1112 - val_accuracy: 0.2065\n",
            "Epoch 20/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.8346 - accuracy: 0.2798 - val_loss: 2.0751 - val_accuracy: 0.2140\n",
            "Epoch 21/25\n",
            "375/375 [==============================] - 11s 31ms/step - loss: 1.7979 - accuracy: 0.2881 - val_loss: 2.1114 - val_accuracy: 0.2151\n",
            "Epoch 22/25\n",
            "375/375 [==============================] - 11s 31ms/step - loss: 1.7823 - accuracy: 0.2983 - val_loss: 2.1490 - val_accuracy: 0.2194\n",
            "Epoch 23/25\n",
            "375/375 [==============================] - 13s 36ms/step - loss: 1.7597 - accuracy: 0.3025 - val_loss: 2.1731 - val_accuracy: 0.2140\n",
            "Epoch 24/25\n",
            "375/375 [==============================] - 14s 38ms/step - loss: 1.7394 - accuracy: 0.3078 - val_loss: 2.1980 - val_accuracy: 0.2229\n",
            "Epoch 25/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 1.7571 - accuracy: 0.3052 - val_loss: 2.2181 - val_accuracy: 0.2097\n",
            "saving model at /content/models/model_skin_tone.h5\n",
            "Training model for gender\n",
            "Epoch 1/25\n",
            "375/375 [==============================] - 20s 36ms/step - loss: 0.7783 - accuracy: 0.5657 - val_loss: 0.5931 - val_accuracy: 0.6656\n",
            "Epoch 2/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.6314 - accuracy: 0.6314 - val_loss: 0.5707 - val_accuracy: 0.6765\n",
            "Epoch 3/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.5974 - accuracy: 0.6611 - val_loss: 0.5592 - val_accuracy: 0.6999\n",
            "Epoch 4/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.5748 - accuracy: 0.6870 - val_loss: 0.5406 - val_accuracy: 0.7050\n",
            "Epoch 5/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.5579 - accuracy: 0.7030 - val_loss: 0.5354 - val_accuracy: 0.7190\n",
            "Epoch 6/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.5385 - accuracy: 0.7172 - val_loss: 0.5475 - val_accuracy: 0.7042\n",
            "Epoch 7/25\n",
            "375/375 [==============================] - 13s 36ms/step - loss: 0.5205 - accuracy: 0.7339 - val_loss: 0.5321 - val_accuracy: 0.7206\n",
            "Epoch 8/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.5119 - accuracy: 0.7438 - val_loss: 0.5443 - val_accuracy: 0.6976\n",
            "Epoch 9/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.4819 - accuracy: 0.7682 - val_loss: 0.5676 - val_accuracy: 0.7186\n",
            "Epoch 10/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.4726 - accuracy: 0.7710 - val_loss: 0.5725 - val_accuracy: 0.6964\n",
            "Epoch 11/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 0.4552 - accuracy: 0.7894 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
            "Epoch 12/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.4399 - accuracy: 0.7967 - val_loss: 0.6143 - val_accuracy: 0.7112\n",
            "Epoch 13/25\n",
            "375/375 [==============================] - 14s 38ms/step - loss: 0.4296 - accuracy: 0.8072 - val_loss: 0.6415 - val_accuracy: 0.7011\n",
            "Epoch 14/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.3991 - accuracy: 0.8211 - val_loss: 0.6385 - val_accuracy: 0.7015\n",
            "Epoch 15/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 0.3761 - accuracy: 0.8348 - val_loss: 0.6807 - val_accuracy: 0.7143\n",
            "Epoch 16/25\n",
            "375/375 [==============================] - 14s 37ms/step - loss: 0.3826 - accuracy: 0.8336 - val_loss: 0.7084 - val_accuracy: 0.6769\n",
            "Epoch 17/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.3633 - accuracy: 0.8415 - val_loss: 0.6813 - val_accuracy: 0.7034\n",
            "Epoch 18/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.3479 - accuracy: 0.8545 - val_loss: 0.7028 - val_accuracy: 0.6984\n",
            "Epoch 19/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 0.3400 - accuracy: 0.8550 - val_loss: 0.7567 - val_accuracy: 0.6871\n",
            "Epoch 20/25\n",
            "375/375 [==============================] - 14s 37ms/step - loss: 0.3332 - accuracy: 0.8585 - val_loss: 0.7611 - val_accuracy: 0.7069\n",
            "Epoch 21/25\n",
            "375/375 [==============================] - 14s 37ms/step - loss: 0.3375 - accuracy: 0.8617 - val_loss: 0.7564 - val_accuracy: 0.6988\n",
            "Epoch 22/25\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.3145 - accuracy: 0.8682 - val_loss: 0.7877 - val_accuracy: 0.7089\n",
            "Epoch 23/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.3056 - accuracy: 0.8779 - val_loss: 0.7625 - val_accuracy: 0.7151\n",
            "Epoch 24/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.3117 - accuracy: 0.8726 - val_loss: 0.7998 - val_accuracy: 0.7108\n",
            "Epoch 25/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 0.2920 - accuracy: 0.8806 - val_loss: 0.8193 - val_accuracy: 0.6995\n",
            "saving model at /content/models/model_gender.h5\n",
            "Training model for age\n",
            "Epoch 1/25\n",
            "375/375 [==============================] - 21s 39ms/step - loss: 1.4384 - accuracy: 0.3907 - val_loss: 1.0582 - val_accuracy: 0.5362\n",
            "Epoch 2/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.1644 - accuracy: 0.4840 - val_loss: 1.0360 - val_accuracy: 0.5390\n",
            "Epoch 3/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.0987 - accuracy: 0.5143 - val_loss: 1.0198 - val_accuracy: 0.5398\n",
            "Epoch 4/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 1.0692 - accuracy: 0.5296 - val_loss: 1.0166 - val_accuracy: 0.5394\n",
            "Epoch 5/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.0459 - accuracy: 0.5415 - val_loss: 1.0118 - val_accuracy: 0.5429\n",
            "Epoch 6/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.0276 - accuracy: 0.5484 - val_loss: 1.0083 - val_accuracy: 0.5472\n",
            "Epoch 7/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 1.0131 - accuracy: 0.5575 - val_loss: 1.0129 - val_accuracy: 0.5460\n",
            "Epoch 8/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 1.0093 - accuracy: 0.5572 - val_loss: 1.0145 - val_accuracy: 0.5503\n",
            "Epoch 9/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 1.0053 - accuracy: 0.5522 - val_loss: 1.0109 - val_accuracy: 0.5553\n",
            "Epoch 10/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.9765 - accuracy: 0.5707 - val_loss: 1.0293 - val_accuracy: 0.5433\n",
            "Epoch 11/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 0.9629 - accuracy: 0.5794 - val_loss: 1.0313 - val_accuracy: 0.5472\n",
            "Epoch 12/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.9497 - accuracy: 0.5791 - val_loss: 1.0492 - val_accuracy: 0.5409\n",
            "Epoch 13/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.9420 - accuracy: 0.5816 - val_loss: 1.0555 - val_accuracy: 0.5433\n",
            "Epoch 14/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.9174 - accuracy: 0.5963 - val_loss: 1.0754 - val_accuracy: 0.5390\n",
            "Epoch 15/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.8982 - accuracy: 0.6040 - val_loss: 1.0764 - val_accuracy: 0.5386\n",
            "Epoch 16/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.8802 - accuracy: 0.6212 - val_loss: 1.1077 - val_accuracy: 0.5433\n",
            "Epoch 17/25\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.8484 - accuracy: 0.6272 - val_loss: 1.1341 - val_accuracy: 0.5374\n",
            "Epoch 18/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.8561 - accuracy: 0.6285 - val_loss: 1.1309 - val_accuracy: 0.5304\n",
            "Epoch 19/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 0.8369 - accuracy: 0.6414 - val_loss: 1.1638 - val_accuracy: 0.5390\n",
            "Epoch 20/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.8090 - accuracy: 0.6434 - val_loss: 1.1912 - val_accuracy: 0.5405\n",
            "Epoch 21/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.7924 - accuracy: 0.6593 - val_loss: 1.2065 - val_accuracy: 0.5316\n",
            "Epoch 22/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.7637 - accuracy: 0.6790 - val_loss: 1.2297 - val_accuracy: 0.5362\n",
            "Epoch 23/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.7577 - accuracy: 0.6773 - val_loss: 1.2447 - val_accuracy: 0.5199\n",
            "Epoch 24/25\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.7348 - accuracy: 0.6863 - val_loss: 1.3087 - val_accuracy: 0.5265\n",
            "Epoch 25/25\n",
            "375/375 [==============================] - 14s 36ms/step - loss: 0.7435 - accuracy: 0.6870 - val_loss: 1.3244 - val_accuracy: 0.5327\n",
            "saving model at /content/models/model_age.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load labels data\n",
        "TESTPATH = '/content/dataset/test/'\n",
        "df_test = pd.read_csv(TESTPATH+'labels.csv')\n",
        "\n",
        "# Convert labels to np array\n",
        "print(\"Converting test labels to np array\")\n",
        "testY = []\n",
        "for i in range(3):\n",
        "    lab = lbs[i].fit_transform(df_test[cat[i]])\n",
        "    if lab.shape[1]==1:\n",
        "        testY.append(np.hstack((1-lab,lab)))\n",
        "    else:\n",
        "        testY.append(lab)\n",
        "        \n",
        "# load and convert images into np array\n",
        "print(\"Loading test images\")\n",
        "nt = df_test.shape[0]\n",
        "all_imgs = [image.load_img(TESTPATH+df_test.iloc[i]['name'], target_size=(length,width)) for i in range(nt)]\n",
        "\n",
        "print(\"Converting test images to np array\")\n",
        "testX = np.empty([nt, length, width, 3], dtype=float)\n",
        "for i in range(nt):\n",
        "    testX[i,:] = image.img_to_array(all_imgs[i])\n",
        "testX = K.applications.resnet_v2.preprocess_input(testX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-g0dtc1bQ4P",
        "outputId": "557c6689-4c33-4d3c-e492-190cdbfec0e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting test labels to np array\n",
            "Loading test images\n",
            "Converting test images to np array\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = mymodel.predict(testX)\n",
        "predY = [[np.argmax(pred[i][j,:]) for j in range(nt)] for i in range(3)]\n",
        "predLabels = [[lbs[i].classes_[j] for j in predY[i]] for i in range(3)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XNefOcrbXmG",
        "outputId": "32839a78-be66-449c-9f66-760d909861bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 5s 30ms/step\n",
            "94/94 [==============================] - 4s 26ms/step\n",
            "94/94 [==============================] - 4s 25ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy\n",
        "acc = {}\n",
        "for i in range(3):\n",
        "    icat = cat[i]\n",
        "    iacc = accuracy_score(df_test[cat[i]], predLabels[i])\n",
        "    acc[icat] = iacc\n",
        "\n",
        "# calculate disparity\n",
        "def disparity_score(ytrue, ypred):\n",
        "    cm = confusion_matrix(ytrue,ypred)\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    all_acc = list(cm.diagonal())\n",
        "    return max(all_acc) - min(all_acc)\n",
        "\n",
        "disp = {}\n",
        "for i in range(3):\n",
        "    icat = cat[i]\n",
        "    idisp = disparity_score(df_test[cat[i]], predLabels[i])\n",
        "    disp[icat] = idisp\n",
        "disp\n",
        "\n",
        "results = {'accuracy': acc, 'disparity': disp}\n",
        "results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdbd2uM0bYBc",
        "outputId": "8a2fa374-f1b3-4b77-8f84-3df51f7b9fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': {'skin_tone': 0.22666666666666666,\n",
              "  'gender': 0.7383333333333333,\n",
              "  'age': 0.5346666666666666},\n",
              " 'disparity': {'skin_tone': 0.41467889908256883,\n",
              "  'gender': 0.09195522544199186,\n",
              "  'age': 0.6112695867443104}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getScore(results):\n",
        "    acc = results['accuracy']\n",
        "    disp = results['disparity']\n",
        "    ad = 2*acc['gender']*(1-disp['gender']) + 4*acc['age']*(1-disp['age']**2) + 10*acc['skin_tone']*(1-disp['skin_tone']**5)\n",
        "    return ad\n",
        "\n",
        "title = '8-Bit Bias Bounty Baseline'\n",
        "    \n",
        "submission = {\n",
        "    'submission_name': title,\n",
        "    'score': getScore(results),\n",
        "    'metrics': results\n",
        "}\n",
        "submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0twNHvlbZUw",
        "outputId": "b5da2ec6-7bee-4190-e3d3-fd2817447c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'submission_name': '8-Bit Bias Bounty Baseline',\n",
              " 'score': 4.9193051354942385,\n",
              " 'metrics': {'accuracy': {'skin_tone': 0.22666666666666666,\n",
              "   'gender': 0.7383333333333333,\n",
              "   'age': 0.5346666666666666},\n",
              "  'disparity': {'skin_tone': 0.41467889908256883,\n",
              "   'gender': 0.09195522544199186,\n",
              "   'age': 0.6112695867443104}}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"evad_score.json\", \"w\") as f:\n",
        "    json.dump(submission, f, indent=4)"
      ],
      "metadata": {
        "id": "5XL-HPlD9DVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}